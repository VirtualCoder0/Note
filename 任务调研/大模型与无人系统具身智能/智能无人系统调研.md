### 01. LLM指令控制无人系统 问题调研

#### a. 相关研究

> **严格来说大语言模型不是直接控制机器人，而是 [LLM for Robotics]([ChatGPT for Robotics](https://www.microsoft.com/en-us/research/articles/chatgpt-for-robotics))，以某种中间层的形式连接到底层的policy，最终还是底层的policy控制机器人。**

##### ChatGPT for Robotics: Design Principles and Model Abilities   ([ChatGPT for Robotics](https://www.microsoft.com/en-us/research/articles/chatgpt-for-robotics))

- 任务输入：详细的场景和任务的自然语言描述

  <img src="https://raw.githubusercontent.com/VirtualCoder0/tuchuang/main/desktop/image-20250219195946038.png" alt="image-20250219195946038" style="zoom:50%;" />

  <img src="https://raw.githubusercontent.com/VirtualCoder0/tuchuang/main/desktop/image-20250219200011049.png" alt="image-20250219200011049" style="zoom:50%;" />

- LLM 输出：基于预定义的规则输出机器人控制脚本

- 该工作的意义在于解决了此前机器人控制研究中需要专业工程师人工将任务目标翻译为控制系统代码的问题，ChatGPT for Robotics 仅需要普通用户为大模型提供高纬度的机器人状态反馈就能基于定义好的规则让 LLM 输出机器人控制脚本，进一步完成预期任务。

  ![robotics today versus with chatgpt](https://www.microsoft.com/en-us/research/uploads/prod/2023/02/main.jpg)

- 该工作并未实现无人系统全流程的自动化实现，过程中仍需要用户对机器人在场景中的情况进行描述。实质上是利用了 LLM 的零样本学习能力，使用对话交互，一步一步地将high-level任务分解为给定API

  - 局限性：**没有解决low-level control** 问题，底层的执行需要用到传统控制方法（比如模型预测方法[MPC](https://zhida.zhihu.com/search?content_id=233038879&content_type=Article&match_order=1&q=MPC&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NDAxMDUxMDgsInEiOiJNUEMiLCJ6aGlkYV9zb3VyY2UiOiJlbnRpdHkiLCJjb250ZW50X2lkIjoyMzMwMzg4NzksImNvbnRlbnRfdHlwZSI6IkFydGljbGUiLCJtYXRjaF9vcmRlciI6MSwiemRfdG9rZW4iOm51bGx9.JoGgfouLmpf4HYf5wXGc5WGkjXI-RcTp9z0gRf147Ec&zhida_source=entity)），它在某些场景不鲁棒，泛化性不强，而且同样需要大量手工设计。部分机器人的low-level control目前还存在瓶颈
    - ps: 部分机器人（比如自动驾驶的车、轮式机器人、机械臂）这些的low-level control好解决，但是足式机器人、人形机器人的low-level control不好解决

##### 视觉-语言模型（VLM）

- RT-1——即Robotics Transformer 1，将「“语言”和“视觉观察”映射到机器人动作」视为一个序列建模问题，然后使用transformer来学习这个映射

- RT-2（在机器人轨迹数据和互联网级别的视觉语言任务联合微调视觉语言模型的学习方式） **vision-language-action(VLA) 模型** 

  - 多模态大模型PaLM-E + 具备机器人操作能力的RT-1

- Open X-Embodiment数据集与RT-X

- RT-H

- 先泛化后加速最后造数据：RT-Trajectory、SARA-RT、AutoRT

#### b. 问题思考

- 我们需要实现的任务场景中，任务的输入是什么形式？
  - 使用的模型是语言模型还是多模态模型？
  - 语言模型：工程量较小，但当前许多研究需要人工描述任务场景中的情况作为 LLM 的行为反馈
  - 视觉模型的优点：可以实现开放式词汇的视觉识别，可以减少流程中人工参与度
  
- 是否具有无人系统的仿真系统作为 LLM 输出指令的测试反馈，如何进行有效的测试反馈？
- LLM 无论是使用思维链推理还是使用类似于 DeepSeek R1 的推理模型都需要很长的推理时间，这样的延迟在无人系统中是否可以接受？
- 该研究的工程量较大，可研究部分较多，研究工作应该怎么切入？

#### c. 使用推理模型进行 Planning 测试

> Imagine I am a robot equipped with a camera and a depth sensor. I am trying to perform a task, and you should help me by sending me commands. You are only allowed to give me the following commands: 
>
> - turn(angle): turn the robot by a given number of degrees 
> - move(distance): moves the robot straight forward by a given distance in meters. 
>
> On each step, I will provide you with the objects in the scene as a list of <object name, distance, angle in degrees>. You should reply with commands to finish the task. The distance is in meters, and the direction angle in degrees with respect to the robot's orientation. Negative angles are to the left and positive angles are to the right. If a command is not valid, I will ignore it and ask you for another command. If there is no relevant information in the scene, use the available commands to explore the environment. 
>
> Task: go to the chairs 
>
> Objects in the scene: <door, 0.53 m, 22>, <chair, 4.84 m, -21>, <chair, 2.12 m, -24>, <window, 6.74 m, -3>, <window, 7.17 m, 6>, <door, 0.61 m, 18>, <stairs, 1.42 m, 0>

##### DeepSeek R1

<img src="https://raw.githubusercontent.com/VirtualCoder0/tuchuang/main/img/image-20250219204203323.png" alt="image-20250219204203323" style="zoom: 50%;" />

First, turn towards the nearest chair to align with it, then move directly towards it:

1. `turn(-24)`  
   (Adjust the robot's orientation to face the chair at -24 degrees)

2. `move(2.12)`  
   (Move straight 2.12 meters to reach the closest chair)

After executing these commands, the robot will arrive at the nearest chair. If additional chairs need to be reached, provide an updated scene description for further commands.

##### DeepSeek V3

To complete the task of going to the chairs, I will guide you to the closest chair first. Here are the commands:

1. **Turn** to face the closest chair:  
   `turn(-24)`  
   (This aligns you with the chair at 2.12 meters and -24 degrees.)

2. **Move** toward the chair:  
   `move(2.12)`  
   (This moves you straight to the chair.)

Let me know when you've executed these commands or if there are any changes in the scene!

- 在选择决策模型时似乎并非模型参数越大效果越好，R1 对比 V3 时发现 R1 存在过度思考的问题，最终 R1 在发现场景中没有目标物体-楼梯时，R1认为它此前一分钟的思考没有意义，并且由于提示词中说如果命令无效用户会忽略命令，所以 R1 认为它可以任意转向进行探索。

  ![image-20250219211610114](https://raw.githubusercontent.com/VirtualCoder0/tuchuang/main/img/image-20250219211610114.png)

- 对比 V3 和 R1-Distill-Qwen-32B 发现，32B 模型的策略更高明，它选择向障碍物少的方向移动来探索环境中的物体，V3 模型选择探索物体列表中没有标明的物体是什么。

  ![image-20250219211503044](https://raw.githubusercontent.com/VirtualCoder0/tuchuang/main/img/image-20250219211503044.png)
